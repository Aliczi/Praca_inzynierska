{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = False\n",
    "if prepare == True:\n",
    "    !pip install pytextrank\n",
    "    !python -m spacy download pl_core_news_sm #or lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytextrank\n",
    "import spacy\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pl = \"Wikipedia – wielojęzyczna encyklopedia internetowa działająca zgodnie z zasadą otwartej treści. Funkcjonuje w oparciu o oprogramowanie MediaWiki (haw. wiki – „szybko”, „prędko”), wywodzące się z koncepcji WikiWikiWeb, umożliwiające edycję każdemu użytkownikowi odwiedzającemu stronę i aktualizację jej treści w czasie rzeczywistym. Słowo Wikipedia jest neologizmem powstałym w wyniku połączenia wyrazów wiki i encyklopedia. Slogan Wikipedii brzmi: „Wolna encyklopedia, którą każdy może redagować”. Serwis był notowany w rankingu Alexa na miejscu 13[1]. \"\n",
    "text_en = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytextrank\n",
    "https://pypi.org/project/pytextrank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English version from textrank example\n",
    "if prepare == True:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nlp.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| phrase: Phrase(text='mixed types', chunks=[mixed types], count=1, rank=0.18359439311764025)\n",
      "ic| phrase: Phrase(text='systems', chunks=[systems, systems, systems], count=3, rank=0.1784796193107821)\n",
      "ic| phrase: Phrase(text='minimal generating sets', chunks=[minimal generating sets], count=1, rank=0.15037838042245094)\n",
      "ic| phrase: Phrase(text='nonstrict inequations', chunks=[nonstrict inequations], count=1, rank=0.14740065982407313)\n",
      "ic| phrase: Phrase(text='strict inequations', chunks=[strict inequations], count=1, rank=0.13946027725597837)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text_en)\n",
    "\n",
    "pytextrank_result_en = doc._.phrases\n",
    "\n",
    "# examine top-ranked phrases in the document\n",
    "for phrase in pytextrank_result_en[:5]:\n",
    "    ic(phrase)\n",
    "\n",
    "tr = doc._.textrank\n",
    "\n",
    "# if prepare == True:\n",
    "#     !pip install \"altair\"\n",
    "#     !pip install 'pytextrank[viz]'\n",
    "# tr.plot_keyphrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| chunk: Compatibility\n",
      "ic| chunk: systems\n",
      "ic| chunk: linear constraints\n",
      "ic| chunk: the set\n",
      "ic| chunk: natural numbers\n",
      "ic| chunk: Criteria\n",
      "ic| chunk: compatibility\n",
      "ic| chunk: a system\n",
      "ic| chunk: linear Diophantine equations\n",
      "ic| chunk: strict inequations\n",
      "ic| chunk: nonstrict inequations\n",
      "ic| chunk: Upper bounds\n",
      "ic| chunk: components\n",
      "ic| chunk: a minimal set\n",
      "ic| chunk: solutions\n",
      "ic| chunk: algorithms\n",
      "ic| chunk: construction\n",
      "ic| chunk: minimal generating sets\n",
      "ic| chunk: solutions\n",
      "ic| chunk: all types\n",
      "ic| chunk: systems\n",
      "ic| chunk: These criteria\n",
      "ic| chunk: the corresponding algorithms\n",
      "ic| chunk: a minimal supporting set\n",
      "ic| chunk: solutions\n",
      "ic| chunk: all the considered types systems\n",
      "ic| chunk: systems\n",
      "ic| chunk: mixed types\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    ic(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def get_chunks(doc):\n",
    "    ## For slovak language\n",
    "    #TODO: change to polish\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    nlp = spacy.load('pl_core_news_sm') \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{POS: 'ADJ', \"OP\": \"+\"}, {POS: {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}]\n",
    "    matcher.add(\"Adjective(s), (p)noun\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        yield start, end, np_label      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# © https://github.com/explosion/spaCy/discussions/7006\n",
    "def is_np_root(word, np_deps, conj):\n",
    "    if word.dep in np_deps:\n",
    "        return True\n",
    "    elif word.dep == conj:\n",
    "        head = word.head\n",
    "        while head.dep == conj and head.head.i < head.i:\n",
    "            head = head.head\n",
    "        return head.dep in np_deps\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def noun_chunks_pl(doclike):\n",
    "    labels = [\n",
    "        \"ROOT\",\n",
    "        \"nsubj\",\n",
    "        \"appos\",\n",
    "        \"nsubjpass\",\n",
    "        \"iobj\",\n",
    "        \"obj\",\n",
    "        \"obl\",\n",
    "        \"obl:arg\",\n",
    "    ]\n",
    "    mod_labels = [\n",
    "        \"amod\",\n",
    "        \"nmod\"\n",
    "    ]\n",
    "    doc = doclike.doc\n",
    "\n",
    "    if not doc.has_annotation(\"DEP\"):\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    mod_deps = [doc.vocab.strings.add(label) for label in mod_labels]\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    prev_end = 0\n",
    "    for i, word in enumerate(doclike):\n",
    "        if word.pos_ not in (\"NOUN\", \"PROPN\"): #TODO PRONs are mostly stop words, should I include it?\n",
    "            continue\n",
    "        if is_np_root(word, np_deps, conj):\n",
    "            start = word.i\n",
    "            end = start + 1\n",
    "            while start >= prev_end and doc[start-1].head in [doc[start], word] and doc[start-1].dep in mod_deps:\n",
    "                start-=1\n",
    "            while doc[end].head in [doc[end-1], word] and doc[end].dep in mod_deps:\n",
    "                end+=1\n",
    "            prev_end = end\n",
    "            yield start, end, np_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'tagger',\n",
       " 'attribute_ruler',\n",
       " 'ner',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the tokens including their lemmas and POS tags\n",
    "# spacy_udpipe.download(\"pl\") # download model\n",
    "nlp_pl = spacy.load('pl_core_news_sm') # or lg\n",
    "spacy.lang.pl.PolishDefaults.syntax_iterators = {\"noun_chunks\" : noun_chunks_pl}  #noun_chunk replacement\n",
    "\n",
    "nlp_pl = spacy.load('pl_core_news_sm') # or lg\n",
    "# for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "# nlp_pl = spacy_udpipe.load(\"pl\")\n",
    "\n",
    "nlp_pl.add_pipe(\"textrank\")#, config={ \"stopwords\": { \"strona\": [\"NOUN\"] } })\n",
    "nlp_pl.pipe_names\n",
    "\n",
    "\n",
    "# https://derwen.ai/docs/ptr/sample/#scrubber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "wielojęzyczna encyklopedia internetowa\n",
      "zasadą\n",
      "oprogramowanie\n",
      "MediaWiki\n",
      "koncepcji WikiWikiWeb\n",
      "edycję każdemu\n",
      "użytkownikowi odwiedzającemu\n",
      "stronę\n",
      "Słowo\n",
      "Wikipedia\n",
      "neologizmem powstałym\n",
      "połączenia\n",
      "wyrazów wiki\n",
      "encyklopedia\n",
      "Slogan Wikipedii\n",
      "rankingu\n",
      "miejscu 13[1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Phrase(text='użytkownikowi odwiedzającemu', chunks=[użytkownikowi odwiedzającemu], count=1, rank=0.12125113702269943),\n",
       " Phrase(text='wyrazów wiki', chunks=[wyrazów wiki], count=1, rank=0.11247190710364127),\n",
       " Phrase(text='koncepcji WikiWikiWeb', chunks=[koncepcji WikiWikiWeb], count=1, rank=0.11138017889935081),\n",
       " Phrase(text='wielojęzyczna encyklopedia internetowa', chunks=[wielojęzyczna encyklopedia internetowa], count=1, rank=0.10994803207631781),\n",
       " Phrase(text='encyklopedia', chunks=[encyklopedia], count=1, rank=0.09912956716674111),\n",
       " Phrase(text='stronę', chunks=[stronę], count=1, rank=0.09307987913442163),\n",
       " Phrase(text='zasadą', chunks=[zasadą], count=1, rank=0.07823278710673826),\n",
       " Phrase(text='neologizmem powstałym', chunks=[neologizmem powstałym], count=1, rank=0.07156333349268114),\n",
       " Phrase(text='połączenia', chunks=[połączenia], count=1, rank=0.06656813718222666),\n",
       " Phrase(text='Alexa', chunks=[Alexa], count=1, rank=0.0643230111160754),\n",
       " Phrase(text='Slogan Wikipedii', chunks=[Slogan Wikipedii], count=1, rank=0.06341967245603825),\n",
       " Phrase(text='MediaWiki', chunks=[MediaWiki], count=1, rank=0.061041910769416736),\n",
       " Phrase(text='oprogramowanie', chunks=[oprogramowanie], count=1, rank=0.05388362546327769),\n",
       " Phrase(text='rankingu', chunks=[rankingu], count=1, rank=0.05388362546327769),\n",
       " Phrase(text='Wikipedia', chunks=[Wikipedia, Wikipedia, Wikipedia, Wikipedia], count=4, rank=0.052553914024320754),\n",
       " Phrase(text='Wikipedii', chunks=[Wikipedii], count=1, rank=0.052553914024320754),\n",
       " Phrase(text='Słowo', chunks=[Słowo], count=1, rank=0.04198687690171032),\n",
       " Phrase(text='edycję każdemu', chunks=[edycję każdemu], count=1, rank=0.04083994411693933),\n",
       " Phrase(text='miejscu 13[1', chunks=[miejscu 13[1], count=1, rank=0.039068776282174145)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_pl(text_pl)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "# for word in doc:\n",
    "#     print(word, type(word.pos_))\n",
    "\n",
    "doc._.phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_EDGE_WEIGHT',\n",
       " '_POS_KEPT',\n",
       " '_TOKEN_LOOKBACK',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_load_stopwords',\n",
       " 'edge_weight',\n",
       " 'pos_kept',\n",
       " 'scrubber',\n",
       " 'stopwords',\n",
       " 'token_lookback']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_object= dict(nlp_pl.pipeline)['textrank']\n",
    "dir(textrank_object)\n",
    "\n",
    "# textrank_object.token_lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Wikipedia SUBST PROPN 0.0\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.tag_, token.pos_, token.sentiment)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summa\n",
    "https://pypi.org/project/summa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare == True:\n",
    "    !pip install summa\n",
    "\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English summma – comparision with pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_result_en = keywords.keywords(text_en, words=5, split=True, scores=True)\n",
    "\n",
    "pytextrank_result_en_list = [(phrase.text, phrase.rank) for phrase in pytextrank_result_en] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('types', 0.3532248258785909),\n",
       " ('numbers', 0.29945212418790307),\n",
       " ('upper', 0.24567942249721883),\n",
       " ('minimal', 0.24228720922768252),\n",
       " ('set', 0.24228720922768227),\n",
       " ('sets', 0.24228720922768227)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mixed types', 0.18359439311764025),\n",
       " ('systems', 0.1784796193107821),\n",
       " ('minimal generating sets', 0.15037838042245094),\n",
       " ('nonstrict inequations', 0.14740065982407313),\n",
       " ('strict inequations', 0.13946027725597837),\n",
       " ('linear Diophantine equations', 0.1195023546245721),\n",
       " ('natural numbers', 0.11450088293222845),\n",
       " ('solutions', 0.10780718173686318),\n",
       " ('linear constraints', 0.10529828014583348),\n",
       " ('all the considered types systems', 0.1036960590708142)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytextrank_result_en_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diophantine equation', 0.4260698809690522),\n",
       " ('strict inequation', 0.42606988096904974),\n",
       " ('linear constraint', 0.31955241072679164),\n",
       " ('nonstrict', 0.2130349404845244),\n",
       " ('consider', 0.016514525023541175),\n",
       " ('type', 0.016514525023541077),\n",
       " ('corresponding', 0.014030102458728334),\n",
       " ('algorithm', 0.014030102458728322),\n",
       " ('upper bound', 0.012385893767655882),\n",
       " ('number', 0.011726607692410565),\n",
       " ('mixed', 0.008257262511770534)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_en)\n",
    "\n",
    "lemmatized_text_en = \" \".join([token.lemma_ for token in doc])\n",
    "summa_result_en = keywords.keywords(lemmatized_text_en, words=15, split=True, scores=True)\n",
    "summa_result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski – summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia – wielojęzyczna encyklopedia internetowa działająca zgodnie z zasadą otwartej treści. Funkcjonuje w oparciu o oprogramowanie MediaWiki (haw. wiki – „szybko”, „prędko”), wywodzące się z koncepcji WikiWikiWeb, umożliwiające edycję każdemu użytkownikowi odwiedzającemu stronę i aktualizację jej treści w czasie rzeczywistym. Słowo Wikipedia jest neologizmem powstałym w wyniku połączenia wyrazów wiki i encyklopedia. Slogan Wikipedii brzmi: „Wolna encyklopedia, którą każdy może redagować”. Serwis był notowany w rankingu Alexa na miejscu 13[1]. \n",
      "\n",
      "Wikipedia – wielojęzyczny encyklopedia internetowy działać zgodnie z zasada otwarty treść . Funkcjonuje w oparcie o oprogramować mediaWiki ( haw . wik – „ szybko ” , „ prędko ” ) , wywodzące się z koncepcja WikiWikiWeb , umożliwiać edycja każdy użytkownik odwiedzający strona i aktualizacja on treść w czas rzeczywisty . słowo Wikipedia być neologizm powstały w wynik połączć wyraz wik i encyklopedia . slogan Wikipedia brzmieć : „ wolny encyklopedia , który każdy móc redagować ” . serwis być notować w ranking Alexa na miejsce 13[1 ] .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('pl_core_news_sm') # or lg\n",
    "doc = nlp(text_pl)\n",
    "\n",
    "lemmatized_text_pl = \" \".join([token.lemma_ for token in doc])\n",
    "print(text_pl, lemmatized_text_pl, sep = \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia',\n",
       " 'encyklopedia',\n",
       " 'wik',\n",
       " 'redagować',\n",
       " 'odwiedzający',\n",
       " 'otwarty treść',\n",
       " 'wikiwikiweb',\n",
       " 'umożliwiać',\n",
       " 'mediawiki']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.keywords(lemmatized_text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encyklopedia',\n",
       " 'wiki',\n",
       " 'treści',\n",
       " 'otwartej',\n",
       " 'odwiedzającemu',\n",
       " 'wikiwikiweb',\n",
       " 'rzeczywistym',\n",
       " 'prędko',\n",
       " 'połączenia',\n",
       " 'mediawiki']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.keywords(text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d3863402639cdbd82538c3facef098fb01dafef1eeee71702a573f758283a3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('jup_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
