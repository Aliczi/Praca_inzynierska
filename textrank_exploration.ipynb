{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = False\n",
    "if prepare == True:\n",
    "    !pip install pytextrank\n",
    "    !python -m spacy download pl_core_news_sm #or lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytextrank\n",
    "import spacy\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pl = \"Wikipedia – wielojęzyczna encyklopedia internetowa działająca zgodnie z zasadą otwartej treści. Funkcjonuje w oparciu o oprogramowanie MediaWiki (haw. wiki – „szybko”, „prędko”), wywodzące się z koncepcji WikiWikiWeb, umożliwiające edycję każdemu użytkownikowi odwiedzającemu stronę i aktualizację jej treści w czasie rzeczywistym. Słowo Wikipedia jest neologizmem powstałym w wyniku połączenia wyrazów wiki i encyklopedia. Slogan Wikipedii brzi: „Wolna encyklopedia, którą każdy może redagować”. Serwis był notowany w rankingu Alexa na miejscu 13[1]. \"\n",
    "text_en = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytextrank\n",
    "https://pypi.org/project/pytextrank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English version from textrank example\n",
    "if prepare == True:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nlp.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| phrase: Phrase(text='mixed types', chunks=[mixed types], count=1, rank=0.18359439311764025)\n",
      "ic| phrase: Phrase(text='systems', chunks=[systems, systems, systems], count=3, rank=0.1784796193107821)\n",
      "ic| phrase: Phrase(text='minimal generating sets', chunks=[minimal generating sets], count=1, rank=0.15037838042245094)\n",
      "ic| phrase: Phrase(text='nonstrict inequations', chunks=[nonstrict inequations], count=1, rank=0.14740065982407313)\n",
      "ic| phrase: Phrase(text='strict inequations', chunks=[strict inequations], count=1, rank=0.13946027725597837)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-787886b3001b470793673433d129af26\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-787886b3001b470793673433d129af26\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-787886b3001b470793673433d129af26\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-697c8ab16a7e66d05160fc56585d7311\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"text\", \"type\": \"nominal\"}, {\"field\": \"rank\", \"type\": \"quantitative\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"index\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"rank\", \"type\": \"quantitative\"}}, \"title\": \"Keyphrase profile of the document\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-697c8ab16a7e66d05160fc56585d7311\": [{\"index\": 0, \"text\": \"mixed types\", \"count\": 1, \"rank\": 0.18359439311764025}, {\"index\": 1, \"text\": \"systems\", \"count\": 3, \"rank\": 0.1784796193107821}, {\"index\": 2, \"text\": \"minimal generating sets\", \"count\": 1, \"rank\": 0.15037838042245094}, {\"index\": 3, \"text\": \"nonstrict inequations\", \"count\": 1, \"rank\": 0.14740065982407313}, {\"index\": 4, \"text\": \"strict inequations\", \"count\": 1, \"rank\": 0.13946027725597837}, {\"index\": 5, \"text\": \"linear Diophantine equations\", \"count\": 1, \"rank\": 0.1195023546245721}, {\"index\": 6, \"text\": \"natural numbers\", \"count\": 1, \"rank\": 0.11450088293222845}, {\"index\": 7, \"text\": \"solutions\", \"count\": 3, \"rank\": 0.10780718173686318}, {\"index\": 8, \"text\": \"linear constraints\", \"count\": 1, \"rank\": 0.10529828014583348}, {\"index\": 9, \"text\": \"all the considered types systems\", \"count\": 1, \"rank\": 0.1036960590708142}, {\"index\": 10, \"text\": \"a minimal supporting set\", \"count\": 1, \"rank\": 0.08812713074893187}, {\"index\": 11, \"text\": \"linear\", \"count\": 1, \"rank\": 0.08444534702772151}, {\"index\": 12, \"text\": \"a system\", \"count\": 1, \"rank\": 0.08243620500315359}, {\"index\": 13, \"text\": \"a minimal set\", \"count\": 1, \"rank\": 0.07944607954086784}, {\"index\": 14, \"text\": \"algorithms\", \"count\": 1, \"rank\": 0.0763527926213032}, {\"index\": 15, \"text\": \"all types\", \"count\": 1, \"rank\": 0.07593126037016427}, {\"index\": 16, \"text\": \"Diophantine\", \"count\": 1, \"rank\": 0.07309361902551355}, {\"index\": 17, \"text\": \"construction\", \"count\": 1, \"rank\": 0.0702090100898443}, {\"index\": 18, \"text\": \"Upper bounds\", \"count\": 1, \"rank\": 0.060225391238828516}, {\"index\": 19, \"text\": \"the set\", \"count\": 1, \"rank\": 0.05800111772673988}, {\"index\": 20, \"text\": \"components\", \"count\": 1, \"rank\": 0.054251394765316464}, {\"index\": 21, \"text\": \"Compatibility\", \"count\": 1, \"rank\": 0.04516904342912139}, {\"index\": 22, \"text\": \"compatibility\", \"count\": 1, \"rank\": 0.04516904342912139}, {\"index\": 23, \"text\": \"the corresponding algorithms\", \"count\": 1, \"rank\": 0.04435648606848154}, {\"index\": 24, \"text\": \"Criteria\", \"count\": 1, \"rank\": 0.042273783712246285}, {\"index\": 25, \"text\": \"These criteria\", \"count\": 1, \"rank\": 0.01952542432474353}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text_en)\n",
    "\n",
    "pytextrank_result_en = doc._.phrases\n",
    "\n",
    "# examine top-ranked phrases in the document\n",
    "for phrase in pytextrank_result_en[:5]:\n",
    "    ic(phrase)\n",
    "\n",
    "tr = doc._.textrank\n",
    "\n",
    "if prepare == True:\n",
    "    !pip install \"altair\"\n",
    "    !pip install 'pytextrank[viz]'\n",
    "tr.plot_keyphrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| chunk: Compatibility\n",
      "ic| chunk: systems\n",
      "ic| chunk: linear constraints\n",
      "ic| chunk: the set\n",
      "ic| chunk: natural numbers\n",
      "ic| chunk: Criteria\n",
      "ic| chunk: compatibility\n",
      "ic| chunk: a system\n",
      "ic| chunk: linear Diophantine equations\n",
      "ic| chunk: strict inequations\n",
      "ic| chunk: nonstrict inequations\n",
      "ic| chunk: Upper bounds\n",
      "ic| chunk: components\n",
      "ic| chunk: a minimal set\n",
      "ic| chunk: solutions\n",
      "ic| chunk: algorithms\n",
      "ic| chunk: construction\n",
      "ic| chunk: minimal generating sets\n",
      "ic| chunk: solutions\n",
      "ic| chunk: all types\n",
      "ic| chunk: systems\n",
      "ic| chunk: These criteria\n",
      "ic| chunk: the corresponding algorithms\n",
      "ic| chunk: a minimal supporting set\n",
      "ic| chunk: solutions\n",
      "ic| chunk: all the considered types systems\n",
      "ic| chunk: systems\n",
      "ic| chunk: mixed types\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    ic(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def get_chunks(doc):\n",
    "    ## For slovak language\n",
    "    #TODO: change to polish\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    nlp = spacy.load('pl_core_news_sm') \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{POS: 'ADJ', \"OP\": \"+\"}, {POS: {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}]\n",
    "    matcher.add(\"Adjective(s), (p)noun\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        yield start, end, np_label      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.pl.Polish object at 0x7f9013738760>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'tagger',\n",
       " 'attribute_ruler',\n",
       " 'ner',\n",
       " 'textrank']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the tokens including their lemmas and POS tags\n",
    "# spacy_udpipe.download(\"pl\") # download model\n",
    "spacy.lang.pl.PolishDefaults.syntax_iterators = {\"noun_chunks\" : get_chunks}  #noun_chunk replacement\n",
    "\n",
    "nlp_pl = spacy.load('pl_core_news_sm') # or lg\n",
    "print(nlp_pl)\n",
    "# nlp_pl = spacy_udpipe.load(\"pl\")\n",
    "\n",
    "nlp_pl.add_pipe(\"textrank\")\n",
    "nlp_pl.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(text='otwartej treści', chunks=[otwartej treści], count=1, rank=0.15272256587280342),\n",
       " Phrase(text='odwiedzającemu stronę', chunks=[odwiedzającemu stronę], count=1, rank=0.12311659088575572),\n",
       " Phrase(text='umożliwiające edycję', chunks=[umożliwiające edycję], count=1, rank=0.11637146094468781),\n",
       " Phrase(text='Wolna encyklopedia', chunks=[Wolna encyklopedia], count=1, rank=0.1077646940828873),\n",
       " Phrase(text='wielojęzyczna encyklopedia', chunks=[wielojęzyczna encyklopedia], count=1, rank=0.10263387240609723),\n",
       " Phrase(text='Alexa', chunks=[Alexa], count=1, rank=0.0643230111160754),\n",
       " Phrase(text='Wikipedia', chunks=[Wikipedia, Wikipedia], count=2, rank=0.052553914024320754),\n",
       " Phrase(text='Wikipedii', chunks=[Wikipedii], count=1, rank=0.052553914024320754)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp_pl(text_pl)\n",
    "\n",
    "doc._.phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Wikipedia SUBST PROPN 0.0\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.tag_, token.pos_, token.sentiment)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summa\n",
    "https://pypi.org/project/summa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare == True:\n",
    "    !pip install summa\n",
    "\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English summma – comparision with pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_result_en = keywords.keywords(text_en, words=5, split=True, scores=True)\n",
    "\n",
    "pytextrank_result_en_list = [(phrase.text, phrase.rank) for phrase in pytextrank_result_en] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('types', 0.3532248258785909),\n",
       " ('numbers', 0.29945212418790307),\n",
       " ('upper', 0.24567942249721883),\n",
       " ('minimal', 0.24228720922768252),\n",
       " ('set', 0.24228720922768227),\n",
       " ('sets', 0.24228720922768227)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa_result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mixed types', 0.18359439311764025),\n",
       " ('systems', 0.1784796193107821),\n",
       " ('minimal generating sets', 0.15037838042245094),\n",
       " ('nonstrict inequations', 0.14740065982407313),\n",
       " ('strict inequations', 0.13946027725597837),\n",
       " ('linear Diophantine equations', 0.1195023546245721),\n",
       " ('natural numbers', 0.11450088293222845),\n",
       " ('solutions', 0.10780718173686318),\n",
       " ('linear constraints', 0.10529828014583348),\n",
       " ('all the considered types systems', 0.1036960590708142)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytextrank_result_en_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diophantine equation', 0.4260698809690522),\n",
       " ('strict inequation', 0.42606988096904974),\n",
       " ('linear constraint', 0.31955241072679164),\n",
       " ('nonstrict', 0.2130349404845244),\n",
       " ('consider', 0.016514525023541175),\n",
       " ('type', 0.016514525023541077),\n",
       " ('corresponding', 0.014030102458728334),\n",
       " ('algorithm', 0.014030102458728322),\n",
       " ('upper bound', 0.012385893767655882),\n",
       " ('number', 0.011726607692410565),\n",
       " ('mixed', 0.008257262511770534)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_en)\n",
    "\n",
    "lemmatized_text_en = \" \".join([token.lemma_ for token in doc])\n",
    "summa_result_en = keywords.keywords(lemmatized_text_en, words=15, split=True, scores=True)\n",
    "summa_result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski – summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia – wielojęzyczna encyklopedia internetowa działająca zgodnie z zasadą otwartej treści. Funkcjonuje w oparciu o oprogramowanie MediaWiki (haw. wiki – „szybko”, „prędko”), wywodzące się z koncepcji WikiWikiWeb, umożliwiające edycję każdemu użytkownikowi odwiedzającemu stronę i aktualizację jej treści w czasie rzeczywistym. Słowo Wikipedia jest neologizmem powstałym w wyniku połączenia wyrazów wiki i encyklopedia. Slogan Wikipedii brzi: „Wolna encyklopedia, którą każdy może redagować”. Serwis był notowany w rankingu Alexa na miejscu 13[1]. \n",
      "\n",
      "Wikipedia – wielojęzyczny encyklopedia internetowy działać zgodnie z zasada otwarty treść . Funkcjonuje w oparcie o oprogramować mediaWiki ( haw . wik – „ szybko ” , „ prędko ” ) , wywodzące się z koncepcja WikiWikiWeb , umożliwiać edycja każdy użytkownik odwiedzający strona i aktualizacja on treść w czas rzeczywisty . słowo Wikipedia być neologizm powstały w wynik połączć wyraz wik i encyklopedia . slogan Wikipedia brzi : „ wolny encyklopedia , który każdy móc redagować ” . serwis być notować w ranking Alexa na miejsce 13[1 ] .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('pl_core_news_sm') # or lg\n",
    "doc = nlp(text_pl)\n",
    "\n",
    "lemmatized_text_pl = \" \".join([token.lemma_ for token in doc])\n",
    "print(text_pl, lemmatized_text_pl, sep = \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia',\n",
       " 'encyklopedia',\n",
       " 'wik',\n",
       " 'redagować',\n",
       " 'odwiedzający',\n",
       " 'otwarty treść',\n",
       " 'wikiwikiweb',\n",
       " 'umożliwiać',\n",
       " 'mediawiki']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.keywords(lemmatized_text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encyklopedia',\n",
       " 'wiki',\n",
       " 'treści',\n",
       " 'otwartej',\n",
       " 'odwiedzającemu',\n",
       " 'wikiwikiweb',\n",
       " 'rzeczywistym',\n",
       " 'prędko',\n",
       " 'połączenia',\n",
       " 'mediawiki']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.keywords(text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d3863402639cdbd82538c3facef098fb01dafef1eeee71702a573f758283a3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('jup_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
