{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = False\n",
    "if prepare == True:\n",
    "    !pip install pytextrank\n",
    "    !python -m spacy download pl_core_news_sm #or lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytextrank\n",
    "import spacy\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pl = \"Wikipedia – wielojęzyczna encyklopedia internetowa działająca zgodnie z zasadą otwartej treści. Funkcjonuje w oparciu o oprogramowanie MediaWiki (haw. wiki – „szybko”, „prędko”), wywodzące się z koncepcji WikiWikiWeb, umożliwiające edycję każdemu użytkownikowi odwiedzającemu stronę i aktualizację jej treści w czasie rzeczywistym. Słowo Wikipedia jest neologizmem powstałym w wyniku połączenia wyrazów wiki i encyklopedia. Slogan Wikipedii brzmi: „Wolna encyklopedia, którą każdy może redagować”. Serwis był notowany w rankingu Alexa na miejscu 13[1]. \"\n",
    "text_en = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytextrank\n",
    "https://pypi.org/project/pytextrank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English version from textrank example\n",
    "if prepare == True:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    \n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nlp.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text_en)\n",
    "\n",
    "pytextrank_result_en = doc._.phrases\n",
    "\n",
    "# examine top-ranked phrases in the document\n",
    "for phrase in pytextrank_result_en[:5]:\n",
    "    ic(phrase)\n",
    "\n",
    "tr = doc._.textrank\n",
    "\n",
    "# if prepare == True:\n",
    "#     !pip install \"altair\"\n",
    "#     !pip install 'pytextrank[viz]'\n",
    "# tr.plot_keyphrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    ic(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def get_chunks(doc):\n",
    "    ## For slovak language\n",
    "    #TODO: change to polish\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    nlp = spacy.load('pl_core_news_sm') \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{POS: 'ADJ', \"OP\": \"+\"}, {POS: {\"IN\": [\"NOUN\", \"PROPN\"]}, \"OP\": \"+\"}]\n",
    "    matcher.add(\"Adjective(s), (p)noun\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        yield start, end, np_label      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# © https://github.com/explosion/spaCy/discussions/7006\n",
    "def is_np_root(word, np_deps, conj):\n",
    "    if word.dep in np_deps:\n",
    "        return True\n",
    "    elif word.dep == conj:\n",
    "        head = word.head\n",
    "        while head.dep == conj and head.head.i < head.i:\n",
    "            head = head.head\n",
    "        return head.dep in np_deps\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def noun_chunks_pl(doclike):\n",
    "    labels = [\n",
    "        \"ROOT\",\n",
    "        \"nsubj\",\n",
    "        \"appos\",\n",
    "        \"nsubjpass\",\n",
    "        \"iobj\",\n",
    "        \"obj\",\n",
    "        \"obl\",\n",
    "        \"obl:arg\",\n",
    "    ]\n",
    "    mod_labels = [\n",
    "        \"amod\",\n",
    "        \"nmod\"\n",
    "    ]\n",
    "    doc = doclike.doc\n",
    "\n",
    "    if not doc.has_annotation(\"DEP\"):\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    mod_deps = [doc.vocab.strings.add(label) for label in mod_labels]\n",
    "    np_label = doc.vocab.strings.add(\"NP\")\n",
    "    prev_end = 0\n",
    "    for i, word in enumerate(doclike):\n",
    "        if word.pos_ not in (\"NOUN\", \"PROPN\"): #TODO PRONs are mostly stop words, should I include it?\n",
    "            continue\n",
    "        if is_np_root(word, np_deps, conj):\n",
    "            start = word.i\n",
    "            end = start + 1\n",
    "            while start >= prev_end and doc[start-1].head in [doc[start], word] and doc[start-1].dep in mod_deps:\n",
    "                start-=1\n",
    "            while doc[end].head in [doc[end-1], word] and doc[end].dep in mod_deps:\n",
    "                end+=1\n",
    "            prev_end = end\n",
    "            yield start, end, np_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the tokens including their lemmas and POS tags\n",
    "# spacy_udpipe.download(\"pl\") # download model\n",
    "nlp_pl = spacy.load('pl_core_news_sm') # or lg\n",
    "spacy.lang.pl.PolishDefaults.syntax_iterators = {\"noun_chunks\" : noun_chunks_pl}  #noun_chunk replacement\n",
    "\n",
    "nlp_pl = spacy.load('pl_core_news_sm') # or lg\n",
    "# for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "# nlp_pl = spacy_udpipe.load(\"pl\")\n",
    "\n",
    "nlp_pl.add_pipe(\"textrank\")#, config={ \"stopwords\": { \"strona\": [\"NOUN\"] } })\n",
    "nlp_pl.pipe_names\n",
    "\n",
    "\n",
    "# https://derwen.ai/docs/ptr/sample/#scrubber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_pl(text_pl)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)\n",
    "\n",
    "# for word in doc:\n",
    "#     print(word, type(word.pos_))\n",
    "\n",
    "doc._.phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_object= dict(nlp_pl.pipeline)['textrank']\n",
    "dir(textrank_object)\n",
    "\n",
    "# textrank_object.token_lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.tag_, token.pos_, token.sentiment)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summa\n",
    "https://pypi.org/project/summa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare == True:\n",
    "    !pip install summa\n",
    "\n",
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English summma – comparision with pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_result_en = keywords.keywords(text_en, words=5, split=True, scores=True)\n",
    "\n",
    "pytextrank_result_en_list = [(phrase.text, phrase.rank) for phrase in pytextrank_result_en] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa_result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytextrank_result_en_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_en)\n",
    "\n",
    "lemmatized_text_en = \" \".join([token.lemma_ for token in doc])\n",
    "summa_result_en = keywords.keywords(lemmatized_text_en, words=15, split=True, scores=True)\n",
    "summa_result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polski – summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_sm') # or lg\n",
    "doc = nlp(text_pl)\n",
    "\n",
    "lemmatized_text_pl = \" \".join([token.lemma_ for token in doc])\n",
    "print(text_pl, lemmatized_text_pl, sep = \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.keywords(lemmatized_text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.keywords(text_pl, language=\"polish\", words=10).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polemo_official = load_dataset(\"data/polemo2-official/\", \"hotels_text\")\n",
    "df_polemo_official = pd.DataFrame(polemo_official[\"train\"])\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_vector = vectorizer.fit_transform(df_polemo_official['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_results = pd.DataFrame(\n",
    "    zip(*[vectorizer.get_feature_names_out(), vectorizer.idf_]),\n",
    "    columns = [\"word\", \"tf-idf\"]\n",
    "    ).sort_values(\"tf-idf\",  ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_polemo_official.groupby(\"target\")\n",
    "grouped.get_group(1)[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\\n\".join(tf_idf_results.head()[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d3863402639cdbd82538c3facef098fb01dafef1eeee71702a573f758283a3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('jup_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
