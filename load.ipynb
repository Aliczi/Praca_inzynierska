{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emo – emotion\n",
    "\n",
    "# czym jest polemo_out, in i official? Czy mają część wspólną. Jeśli tak, czy etykiety się zgadzają?\n",
    "# Czym różni się zbiór \"hotel_text\" od \"hotel_sentences\" w polemo official? Mają część wspólną?\n",
    "# Czy ocena poszczególnych słów w aspect emo jest taka sama dla różnych opinii? Czy są słowa, które nie mają oceny w żadnej opiniii?\n",
    "# Czy jesteśmy w stanie wykorzystać aspect emo? Uśredniać? Samemu generować tylko oceny dla słów? Wziąć oceny dla słów, które będą naszymi kandydatami na keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polemo_offi_config = pd.read_json('data/polemo2-official/dataset_infos.json') #this is config file\n",
    "polemo_category = \"hotels_text\"\n",
    "polemo_official = load_dataset(\"data/polemo2-official/\", polemo_category) # only oppinions about hotels\n",
    "df_polemo_official = pd.DataFrame(polemo_official[\"train\"])\n",
    "\n",
    "aspectemo = load_dataset(\"data/aspectemo\")\n",
    "df_aspectemo = pd.DataFrame(aspectemo[\"train\"])\n",
    "\n",
    "df_opta_reviews = pd.read_json(\"data/OPTA-treebank-reviews/OPTA-treebank-0.1.json\")\n",
    "df_opta_skladnica = pd.read_json(\"data/OPTA-treebank-skladnica/skladnica_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polemo official\n",
    "\n",
    "Polemo out and polemo in includes in polemo official, so we don't use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available categories of oppinions in polemo\n",
    "list(polemo_offi_config.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polemo_official.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polemo_official.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get meanings of labels in polemo_official based on polemo_in (polemo out has the same meanings)\n",
    "df_polemo_out = pd.read_csv(\"data/klej_polemo2_in/train.csv\")\n",
    "\n",
    "df_polemo_out.rename({'sentence': 'text'}, axis=1, inplace=True)\n",
    "df_intersection = pd.merge(df_polemo_out, df_polemo_official, how='inner', on=['text'])\n",
    "df_intersection.head()\n",
    "\n",
    "df_intersection.drop(\"text\", axis=1, inplace = True)\n",
    "df_intersection.drop_duplicates(\"target_y\", inplace = True)\n",
    "\n",
    "df_intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspectemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_aspectemo.head(3))\n",
    "# [\"O\", \"a_minus_m\", \"a_minus_s\", \"a_zero\", \"a_plus_s\", \"a_plus_m\", \"a_amb\"]\n",
    "\n",
    "example = 2\n",
    "list(zip(df_aspectemo.iloc[example].tokens, df_aspectemo.iloc[example].labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTA reviews\n",
    "\n",
    "The file contains JSON formatted results of annotation of opinions and their targets. \n",
    "\n",
    "Each item in the list is a sentence which contains following fields:\n",
    "\n",
    "'file_id': ID of review (used internally, same sentence may appear multiple times with different sentiments and opinin target pairs) </br>\n",
    "'domain': review type (perfume or clothes) </br>\n",
    "'dist':  dependency path distance between S and T, filled only for the 1st batch of annotations </br>\n",
    "'isSentIncorrect': human annotation - is sentiment word S incorrect? </br>\n",
    "'isAttrIncorrect': human annotation - is opinion target word T incorrect? </br>\n",
    "'parsedSent': CONLL-formatted parsed sentence; last column contains pointers to : </br>\n",
    "\tS = sentiment word </br>\n",
    "\tT = opinion target word </br>\n",
    "'isStrError': human annotation - is dependency structure erroneous between S and T </br>\n",
    "'isAtrRelToSent': human annotation - is S related to T  </br>\n",
    "'rule_id': ID of extraction rule that pointed to T (see \"extraction rules\")\n",
    "\n",
    "read about CONLL format here: https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opta_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_get_word_and_misc(conll: list) -> list:\n",
    "    # based on readme and https://universaldependencies.org/format.html\n",
    "    conll = conll[0].replace(\"\\n\", \"\")\n",
    "    conll_list = conll.split(\"\\t\")[:-1] # last field is '\\n'\n",
    "    return [conll_list[1]] + [conll_list[-1]]\n",
    "\n",
    "def df_from_parsedSent(df: pd.DataFrame):\n",
    "    opta_words = df[\"parsedSent\"].apply(conll_get_word_and_misc)\n",
    "    opta_words = list(zip(*opta_words))\n",
    "    df_opta_words = pd.DataFrame()\n",
    "    df_opta_words[\"word\"] = opta_words[0]\n",
    "    df_opta_words[\"misc\"] = opta_words[1]\n",
    "    return df_opta_words\n",
    "\n",
    "print(\"Example od parsedSent\\n\", df_opta_reviews[\"parsedSent\"][0][0])\n",
    "\n",
    "df_opta_words = df_from_parsedSent(df_opta_reviews)\n",
    "print(\"Words with their labels(?)\\n\", df_opta_words.head(), \"\\n\")\n",
    "\n",
    "print(\"Class distribution for labels(?)\\n\", df_opta_words[\"misc\"].value_counts())\n",
    "\n",
    "# I don't quite understand how this dataset works :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTA składnica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opta_skladnica.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opta_words = df_from_parsedSent(df_opta_skladnica)\n",
    "print(df_opta_words.head())\n",
    "df_opta_words[\"misc\"].value_counts()\n",
    "\n",
    "# Looks familiar to OPTA reviews, there are many _ in last column from parsedSent. I don't think it can be useful for us\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('my_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9a334995f8e6cd01751d04ca80b84c3bca6a349fda10ede2b4337cee1cbaf5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
